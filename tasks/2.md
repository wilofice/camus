Camus - Ollama Integration Action Plan
This document breaks down the Ollama integration into a series of small, actionable, and testable tasks.

Task 1: Add New Dependencies to Project
Objective: Integrate the required C++ libraries for making HTTP requests and parsing JSON into our CMake build system.

Files to Modify:

CMakeLists.txt

Implementation Guidelines:

Add a FetchContent_Declare block for yhirose/cpp-httplib. This will be our HTTP client.

Add a FetchContent_Declare block for nlohmann/json. This will be our JSON parser.

In the target_include_directories section for the camus executable, add the source directories for these two new libraries so our code can find their header files.

Testing / Verification:

After making the changes, run the cmake configuration command. It should complete successfully, downloading the new libraries into the build/_deps directory.

The project should still compile cleanly, even though we aren't using the new libraries yet.

Task 2: Update Configuration File
Objective: Modify the init command to generate a config.yml file that includes the new settings required for Ollama.

Files to Modify:

src/Camus/Core.cpp

Implementation Guidelines:

Locate the handleInit() method in Core.cpp.

Modify the defaultConfig string to include two new keys:

backend: direct (This will be the default).

ollama_url: http://localhost:11434

Testing / Verification:

Delete any existing .camus directory.

Run the command ./build/bin/Debug/camus init.

Open the newly created .camus/config.yml file and verify that the backend and ollama_url lines are present with their default values.

Task 3: Refactor LlmInteraction into an Interface
Objective: Decouple our core logic from the specific llama.cpp implementation by creating an abstract interface. This is a critical prerequisite for supporting multiple backends.

Files to Modify / Create:

Modify: include/Camus/LlmInteraction.hpp (will become the interface).

Create: include/Camus/LlamaCppInteraction.hpp and src/Camus/LlamaCppInteraction.cpp.

Modify: src/Camus/Core.hpp and src/Camus/Core.cpp.

Modify: CMakeLists.txt to include the new .cpp file.

Implementation Guidelines:

In LlmInteraction.hpp, change the class to be an abstract interface with a virtual destructor and a pure virtual getCompletion method: virtual std::string getCompletion(const std::string& prompt) = 0;.

Create the new LlamaCppInteraction class that inherits from LlmInteraction.

Move all the existing implementation code from the old LlmInteraction.cpp into the new LlamaCppInteraction.cpp. Update the class names accordingly.

Update the m_llm member in Core.hpp to be a std::unique_ptr<LlmInteraction>.

In the Core.cpp constructor, for now, instantiate the concrete class: m_llm = std::make_unique<LlamaCppInteraction>(full_model_path);.

Testing / Verification:

After refactoring, ensure your config.yml is set to backend: direct.

Run a modify command. The application should function exactly as it did before. This proves our refactoring was successful and didn't break the existing functionality.

Task 4: Implement the OllamaInteraction Class
Objective: Create the new class that will handle all communication with the Ollama server.

Files to Modify / Create:

Create: include/Camus/OllamaInteraction.hpp and src/Camus/OllamaInteraction.cpp.

Modify: CMakeLists.txt to include the new .cpp file.

Implementation Guidelines:

The OllamaInteraction class must inherit from the LlmInteraction interface.

The constructor should accept the server URL and the model name to be used.

The getCompletion method will contain the core REST API logic:

Include httplib.h and nlohmann/json.hpp.

Create a nlohmann::json object for the request body. It must include the keys model, prompt, and stream: false.

Create an httplib::Client and make a POST request to the /api/generate endpoint.

Check the HTTP response status. If it's not 200, throw a std::runtime_error.

Parse the JSON response body, extract the value of the response key, and return it as a string.

Testing / Verification:

This task is primarily code creation. Its full functionality will be tested in the final step.

Task 5: Implement the Backend Factory in Core
Objective: Tie everything together by making the Core class choose which backend to use based on the configuration file.

Files to Modify:

src/Camus/Core.cpp

Implementation Guidelines:

In the Core constructor, after creating the m_config object, read the value of the backend key.

Use an if/else block:

If backend == "ollama", read the ollama_url and default_model from the config. Instantiate m_llm as std::make_unique<OllamaInteraction>(url, model_name);.

else (or if backend == "direct"), use the existing logic to read the model path and instantiate m_llm as std::make_unique<LlamaCppInteraction>(full_model_path);.

Testing / Verification:

Test Case 1 (Ollama): Set backend: ollama in your config.yml. Start your local Ollama server (ollama serve). Run camus modify .... Verify that the command completes successfully and that the output comes from the Ollama server.

Test Case 2 (Direct): Set backend: direct in your config.yml. Run camus modify .... Verify that the command completes successfully using the direct llama.cpp integration, just as it does now.