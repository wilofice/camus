// =================================================================
// include/Camus/OllamaInteraction.hpp (New File)
// =================================================================
// Defines the interface for interacting with the Ollama server.

#pragma once

#include "Camus/LlmInteraction.hpp"

namespace Camus {

class OllamaInteraction : public LlmInteraction {
public:
    /**
     * @brief Constructs the Ollama client.
     * @param server_url The base URL of the Ollama server (e.g., http://localhost:11434).
     * @param model_name The name of the model to use (e.g., llama3:latest).
     */
    OllamaInteraction(const std::string& server_url, const std::string& model_name);

    /**
     * @brief Sends a prompt to the Ollama server and gets a response.
     * @param prompt The fully-formed prompt to send to the model.
     * @return The text generated by the model.
     */
    std::string getCompletion(const std::string& prompt) override;

private:
    std::string m_server_url;
    std::string m_model_name;
};

} // namespace Camus
