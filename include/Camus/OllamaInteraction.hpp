// =================================================================
// include/Camus/OllamaInteraction.hpp
// =================================================================
// Concrete implementation of LlmInteraction using Ollama server.

#pragma once

#include "Camus/LlmInteraction.hpp"
#include <string>

namespace Camus {

class OllamaInteraction : public LlmInteraction {
public:
    /**
     * @brief Constructor configures the Ollama server connection.
     * @param server_url The URL of the Ollama server (e.g., "http://localhost:11434").
     * @param model_name The name of the model to use (e.g., "llama3").
     */
    OllamaInteraction(const std::string& server_url, const std::string& model_name);
    ~OllamaInteraction() override = default;

    /**
     * @brief Sends a prompt to the Ollama server and gets a response.
     * @param prompt The fully-formed prompt to send to the model.
     * @return The text generated by the model.
     */
    std::string getCompletion(const std::string& prompt) override;

private:
    std::string m_server_url;
    std::string m_model_name;
};

} // namespace Camus