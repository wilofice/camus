// =================================================================
// include/Camus/LlmInteraction.hpp
// =================================================================
// Abstract interface for interacting with LLMs with enhanced metadata support.

#pragma once

#include "Camus/ModelCapabilities.hpp"
#include <string>
#include <memory>
#include <chrono>

namespace Camus {

/**
 * @brief Request configuration for model inference
 */
struct InferenceRequest {
    std::string prompt;                     ///< The input prompt
    size_t max_tokens = 2048;              ///< Maximum tokens to generate
    double temperature = 0.7;              ///< Sampling temperature (0.0-2.0)
    double top_p = 0.9;                    ///< Top-p sampling parameter
    std::vector<std::string> stop_sequences; ///< Stop generation at these sequences
    bool stream = false;                    ///< Whether to stream the response
    std::chrono::milliseconds timeout{30000}; ///< Request timeout
};

/**
 * @brief Response from model inference including metadata
 */
struct InferenceResponse {
    std::string text;                       ///< Generated text
    size_t tokens_generated = 0;           ///< Number of tokens generated
    std::chrono::milliseconds response_time{0}; ///< Time taken for generation
    bool was_truncated = false;            ///< Whether response was truncated
    std::string finish_reason;             ///< Reason for completion (stop, length, error)
    double confidence_score = 0.0;         ///< Model's confidence in response (0.0-1.0)
    std::unordered_map<std::string, std::string> metadata; ///< Additional metadata
};

/**
 * @brief Enhanced abstract interface for LLM interactions
 */
class LlmInteraction {
public:
    virtual ~LlmInteraction() = default;

    /**
     * @brief Sends a prompt to the LLM and gets a response.
     * @param prompt The fully-formed prompt to send to the model.
     * @return The text generated by the model.
     */
    virtual std::string getCompletion(const std::string& prompt) = 0;
    
    /**
     * @brief Enhanced completion method with request/response metadata
     * @param request The inference request with configuration
     * @return The inference response with metadata
     */
    virtual InferenceResponse getCompletionWithMetadata(const InferenceRequest& request) {
        // Default implementation using the basic getCompletion method
        auto start_time = std::chrono::steady_clock::now();
        
        InferenceResponse response;
        response.text = getCompletion(request.prompt);
        
        auto end_time = std::chrono::steady_clock::now();
        response.response_time = std::chrono::duration_cast<std::chrono::milliseconds>(
            end_time - start_time);
        response.finish_reason = "stop";
        
        return response;
    }
    
    /**
     * @brief Get model metadata and capabilities
     * @return Model metadata structure
     */
    virtual ModelMetadata getModelMetadata() const = 0;
    
    /**
     * @brief Check if the model is currently healthy and available
     * @return True if model is ready for inference
     */
    virtual bool isHealthy() const = 0;
    
    /**
     * @brief Perform a health check on the model
     * @return Updated health status
     */
    virtual bool performHealthCheck() = 0;
    
    /**
     * @brief Get current performance metrics
     * @return Current performance statistics
     */
    virtual ModelPerformance getCurrentPerformance() const = 0;
    
    /**
     * @brief Warm up the model (pre-load, cache initialization, etc.)
     * @return True if warmup was successful
     */
    virtual bool warmUp() {
        // Default implementation - no warmup needed
        return true;
    }
    
    /**
     * @brief Clean up resources (unload model, free memory, etc.)
     */
    virtual void cleanup() {
        // Default implementation - no cleanup needed
    }
    
    /**
     * @brief Get a unique identifier for this model instance
     * @return Unique model instance ID
     */
    virtual std::string getModelId() const = 0;
};

} // namespace Camus