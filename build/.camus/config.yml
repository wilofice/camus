# Camus Configuration v1.0
# Backend configuration: 'direct' for llama.cpp or 'ollama' for Ollama server
backend: ollama

# Direct backend settings (when backend: direct)
model_path: /path/to/your/models/
default_model: deepseek-r1

# Ollama backend settings (when backend: ollama)
ollama_url: http://localhost:11434

# Build and test commands
build_command: 'cmake --build ./build'
test_command: 'ctest --test-dir ./build'
